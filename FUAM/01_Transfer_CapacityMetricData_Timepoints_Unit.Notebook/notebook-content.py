# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "607e1068-530b-4df1-912c-750cf336eec2",
# META       "default_lakehouse_name": "FUAM_Lakehouse",
# META       "default_lakehouse_workspace_id": "bd9136d2-e45a-463d-b9be-8d1a10717acb",
# META       "known_lakehouses": [
# META         {
# META           "id": "607e1068-530b-4df1-912c-750cf336eec2"
# META         }
# META       ]
# META     },
# META     "environment": {}
# META   }
# META }

# MARKDOWN ********************

# #### Get Capacity Metrics (Timepoints)
# by Capacity by Day
# 
# ##### Data ingestion strategy:
# <mark style="background: #D69AFE;">**MERGE**</mark>
# 
# ##### Related pipeline:
# 
# **Load_Capacity_Metrics_E2E**
# 
# ##### Source:
# 
# **Capacity Metrics** via SemPy DAX execute query function
# 
# ##### Target:
# 
# **1 Delta table** in FUAM_Lakehouse 
# - **gold_table_name** variable value

# CELL ********************

import sempy.fabric as fabric
from datetime import datetime, timedelta
import datetime as dt
import pyspark.sql.functions as f
from delta.tables import *

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# PARAMETERS CELL ********************

## Parameters
# These parameters will be overwritten while executing the notebook 
# from Load_FUAM_Data_E2E Pipeline
metric_days_in_scope = 2
metric_workspace = ""
metric_dataset = ""
display_data = False

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

## Variables
silver_table_name = "FUAM_Staging_Lakehouse.capacity_metrics_by_timepoint_silver"
gold_table_name = "capacity_metrics_by_timepoint"
gold_table_name_with_prefix = f"Tables/{gold_table_name}"

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Check Table Status
version = ''
  
try: 
      check_table_structure_query = """DEFINE    MPARAMETER 'DefaultCapacityID' = "0000000-0000-0000-0000-00000000"
                                      EVALUATE   SUMMARIZECOLUMNS("Background billable CU %", [Background billable CU %]    )"""
      check_table_structure_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query)
      print("Test for v44 successful")
      version = 'v44'
except:
      print("Test for v44 failed")
  
if version == '':
    try:
        check_table_structure_query = """EVALUATE ROW("Background billable CU %", 'All Measures'[Background billable CU %])"""
        check_table_structure_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query)
        print("Test for v40 successful")
        version = 'v40'
    except:
        print("Test for v40 failed")
  
if version == '':
    try:
        check_table_structure_query_alternative = """EVALUATE ROW("xBackground__", 'All Measures'[xBackground %])"""
        check_table_structure_df_alternative = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query_alternative)
        version = 'v37'
        print("Test for v37 successful")
    except:
        print("Test for v37 failed")
  
  
if version != '':
    print( f'Version {version} is valid')
else: 
    print("ERROR: Capacity Metrics data structure is not compatible or connection to capacity metrics is not possible.")
    exit

# CELL ********************

# Fetch capacities from connected capacity metrics app
if version in ['v44', 'v40']:
  capacity_query = """EVALUATE SELECTCOLUMNS (    Capacities, "capacity Id", Capacities[capacity Id] , "state" , Capacities[state] )"""
else:
  capacity_query = """EVALUATE SELECTCOLUMNS (    Capacities, "capacity Id", Capacities[CapacityId] , "state" , Capacities[state] )"""
capacities = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=capacity_query)
capacities.columns = ['CapacityId', 'State']
capacities = spark.createDataFrame(capacities)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

if display_data:
    display(capacities)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Iterate days
def iterate_dates(start_date, end_date):
    # Init array
    dates = []
    # Convert string inputs to datetime objects
    start = dt.datetime.strptime(start_date, '%Y-%m-%d')
    end = dt.datetime.strptime(end_date, '%Y-%m-%d')
    
    # Initialize current date as start date
    current_date = start.date()
    
    while current_date <= end.date():

        dates.append(
            {
                "date": current_date,
                "year": current_date.year,
                "month": current_date.month,
                "day": current_date.day
            })
        # Move to the next day
        current_date += dt.timedelta(days=1)

    return dates


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Clean Silver table
try:
    query = "DELETE FROM " + silver_table_name
    spark.sql(query)

except Exception as ex:
    print("INFO: Silver table doesn't exist yet.")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Iterate capacities and days
for cap in capacities.collect():
    capacity_id = cap[0]
    
    print(f"INFO: Scoped CapacityId: {capacity_id}")

    try:
        # Get today's date
        today = datetime.now()

        # Calculate the dates between today and days_in_scope
        days_ago = today - timedelta(days=metric_days_in_scope)

        # Format dates in 'yyyy-mm-dd'
        today_str = today.strftime('%Y-%m-%d')
        days_ago_str = days_ago.strftime('%Y-%m-%d')

        date_array = iterate_dates(days_ago_str, end_date=today_str)
        print(f"INFO: Get data for CapacityId: {capacity_id}")

        # Iterate days for current capacity
        for date in date_array:
            year = date['year']
            month = date['month']
            day = date['day']
            date_label = str(year) + '-' + str(month) + '-' + str(day)
            
            dax_query_v44 = f"""
            DEFINE
            
             MPARAMETER 'CapacitiesList' = {{ \"{capacity_id}\" }}

            VAR __DS0Core = 
                    SUMMARIZECOLUMNS(
                        Capacities[capacity Id],
                        'TimePoints'[TimePoint],
                        FILTER(Capacities, Capacities[capacity Id] = \"{capacity_id}\" ),
                        FILTER(TimePoints,  'TimePoints'[Date] = DATE({year}, {month}, {day})),
                        "B_P", 'All Measures'[Background billable CU %],
                        "I_P", 'All Measures'[Interactive billable CU %],
                        "B_NB_P", 'All Measures'[Background non billable CU %],
                        "I_NB_P", 'All Measures'[Interactive non billable CU %],
                        "AS_P", 'All Measures'[SKU CU by timepoint %],
                        "CU_L", 'All Measures'[CU limit],
                        "T_CU_U_P", 'All Measures'[Cumulative CU usage % preview],
                        "C_CU_U_S", 'All Measures'[Cumulative CU usage (s)],
                        "SKU_CU_TP", 'All Measures'[SKU CU by timepoint],
                        "I_Del_P", 'All Measures'[Dynamic interactive delay %],
                        "I_Rej_P", 'All Measures'[Dynamic interactive rejection %],
                        "I_Rej_TH", 'All Measures'[Interactive rejection threshold],
                        "B_Rej_P", 'All Measures'[Dynamic background rejection %],
                        "B_Rej_TH", 'All Measures'[Background rejection threshold],
                        "CO_A_P", 'All Measures'[Carry over add %],
                        "CO_BD_P", 'All Measures'[Carry over burndown %],
                        "CO_C_P", 'All Measures'[Cumulative carry over %],
                        "OV_RL", 'All Measures'[Overage reference line],
                        "Exp_BD_M", 'All Measures'[Expected burndown in minutes]
                    )

            EVALUATE
                __DS0Core
            """


            dax_query_v40 = f"""
            DEFINE
            
            MPARAMETER 'CapacityID' = \"{capacity_id}\"

            VAR __DS0Core = 
                    SUMMARIZECOLUMNS(
                        Capacities[capacity Id],
                        'TimePoints'[TimePoint],
                        FILTER(Capacities, Capacities[capacity Id] = \"{capacity_id}\" ),
                        FILTER(TimePoints,  'TimePoints'[Date] = DATE({year}, {month}, {day})),
                        "B_P", 'All Measures'[Background billable CU %],
                        "I_P", 'All Measures'[Interactive billable CU %],
                        "B_NB_P", 'All Measures'[Background non billable CU %],
                        "I_NB_P", 'All Measures'[Interactive non billable CU %],
                        "AS_P", 'All Measures'[SKU CU by timepoint %],
                        "CU_L", 'All Measures'[CU limit],
                        "T_CU_U_P", 'All Measures'[Cumulative CU usage % preview],
                        "C_CU_U_S", 'All Measures'[Cumulative CU usage (s)],
                        "SKU_CU_TP", 'All Measures'[SKU CU by timepoint],
                        "I_Del_P", 'All Measures'[Dynamic interactive delay %],
                        "I_Rej_P", 'All Measures'[Dynamic interactive rejection %],
                        "I_Rej_TH", 'All Measures'[Interactive rejection threshold],
                        "B_Rej_P", 'All Measures'[Dynamic background rejection %],
                        "B_Rej_TH", 'All Measures'[Background rejection threshold],
                        "CO_A_P", 'All Measures'[Carry over add %],
                        "CO_BD_P", 'All Measures'[Carry over burndown %],
                        "CO_C_P", 'All Measures'[Cumulative carry over %],
                        "OV_RL", 'All Measures'[Overage reference line],
                        "Exp_BD_M", 'All Measures'[Expected burndown in minutes]
                    )

            EVALUATE
                __DS0Core
            """

            dax_query_v37 = f"""
            DEFINE

            MPARAMETER 'CapacityID' = \"{capacity_id}\"

            VAR __DS0Core = 
                    SUMMARIZECOLUMNS(
                        Capacities[capacityId],
                        'TimePoints'[TimePoint],
                        FILTER(Capacities, Capacities[capacityId] = \"{capacity_id}\" ),
                        FILTER(TimePoints,  'TimePoints'[Date] = DATE({year}, {month}, {day})),
                        "B_P", 'All Measures'[xBackground %],
                        "I_P", 'All Measures'[xInteractive %],
                        "B_NB_P", 'All Measures'[xBackground % Preview],
                        "I_NB_P", 'All Measures'[xInteractive % Preview],
                        "AS_P", 'All Measures'[SKU CU by TimePoint %],
                        "CU_L", 'All Measures'[CU Limit],
                        "T_CU_U_P", 'All Measures'[Cumulative CU Usage % Preview],
                        "C_CU_U_S", 'All Measures'[Cumulative CU Usage (s)],
                        "SKU_CU_TP", 'All Measures'[SKU CU by TimePoint],
                        "I_Del_P", 'All Measures'[Dynamic InteractiveDelay %],
                        "I_Rej_P", 'All Measures'[Dynamic InteractiveRejection %],
                        "I_Rej_TH", 'All Measures'[Interactive rejection threshold],
                        "B_Rej_P", 'All Measures'[Dynamic BackgroundRejection %],
                        "B_Rej_TH", 'All Measures'[Background rejection threshold],
                        "CO_A_P", 'All Measures'[xCarryOver_added %],
                        "CO_BD_P", 'All Measures'[xCarryOver_burndown %],
                        "CO_C_P", 'All Measures'[xCarryOver_Cumulative %],
                        "OV_RL", 'All Measures'[Overage reference line],
                        "Exp_BD_M", 'All Measures'[Expected burndown in minutes]
                    )
            EVALUATE
                __DS0Core
            """

            dax_query = ""
            # Choose query
            if version == 'v44':
                dax_query = dax_query_v44
            elif version == 'v40':
                dax_query = dax_query_v40
            elif version == 'v37':
                dax_query = dax_query_v37
                
            # Execute DAX query
            capacity_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=dax_query)
            capacity_df.columns = ['CapacityId', 'TimePoint', 'BackgroundPercentage', 'InteractivePercentage', 
                                    'BackgroundNonBillablePercentage', 'InteractiveNonBillablePercentage', 'AutoscalePercentage', 
                                    'CULimitPercentage', 'TotalCUUsagePercentage', 'TotalCUs', 'SKUCUByTimePoint', 
                                    'InteractiveDelayPercentage', 'InteractiveRejectionPercentage', 'InteractiveRejectionThreshold', 
                                    'BackgroundRejectionPercentage', 'BackgroundRejectionThreshold', 'CarryOverAddedPercentage', 
                                    'CarryOverBurndownPercentage', 'CarryOverCumulativePercentage', 'OverageReferenceLine', 
                                    'ExpectedBurndownInMin']
            
            if not(capacity_df.empty):
                # Transfer pandas df to spark df
                capacity_df = spark.createDataFrame(capacity_df)

                if display_data:
                    display(capacity_df)

                # Write prepared bronze_df to silver delta table
                print(f"INFO: Appending data for CapacityId: {capacity_id} on Date: {date_label}")
                capacity_df.write.mode("append").option("mergeSchema", "true").format("delta").saveAsTable(silver_table_name)  
            else:
                print(f"INFO: No data for CapacityId: {capacity_id} on Date: {date_label}")

    except Exception as ex:
        print(ex)
        continue

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Get Silver table data
query = "SELECT * FROM  " + silver_table_name
silver_df = spark.sql(query)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Check if gold table exists
if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_table_name):
    # if exists -> MERGE to gold
    print("INFO: Gold table exists and will be merged.")

    gold_df = DeltaTable.forPath(spark, gold_table_name_with_prefix)
    # Merge silver (s = source) to gold (t = target)
    gold_df.alias('t') \
    .merge(
        silver_df.alias('s'),
        "s.CapacityId = t.CapacityId AND s.TimePoint = t.TimePoint"
    ) \
    .whenMatchedUpdate(set =
        {
             "BackgroundPercentage": "s.BackgroundPercentage"
            ,"InteractivePercentage": "s.InteractivePercentage"
            ,"BackgroundNonBillablePercentage": "s.BackgroundNonBillablePercentage"
            ,"InteractiveNonBillablePercentage": "s.InteractiveNonBillablePercentage"
            ,"AutoscalePercentage": "s.AutoscalePercentage"
            ,"CULimitPercentage": "s.CULimitPercentage"
            ,"TotalCUUsagePercentage": "s.TotalCUUsagePercentage"
            ,"TotalCUs": "s.TotalCUs"
            ,"SKUCUByTimePoint": "s.SKUCUByTimePoint"
            ,"InteractiveDelayPercentage": "s.InteractiveDelayPercentage"
            ,"InteractiveRejectionPercentage": "s.InteractiveRejectionPercentage"
            ,"InteractiveRejectionThreshold": "s.InteractiveRejectionThreshold"
            ,"BackgroundRejectionPercentage": "s.BackgroundRejectionPercentage"
            ,"BackgroundRejectionThreshold": "s.BackgroundRejectionThreshold"
            ,"CarryOverAddedPercentage": "s.CarryOverAddedPercentage"
            ,"CarryOverBurndownPercentage": "s.CarryOverBurndownPercentage"
            ,"CarryOverCumulativePercentage": "s.CarryOverCumulativePercentage"
            ,"OverageReferenceLine": "s.OverageReferenceLine"
            ,"ExpectedBurndownInMin": "s.ExpectedBurndownInMin"
        }
    ) \
    .whenNotMatchedInsert(values =
        {
             "CapacityId": "s.CapacityId"
            ,"TimePoint": "s.TimePoint"
            ,"BackgroundPercentage": "s.BackgroundPercentage"
            ,"InteractivePercentage": "s.InteractivePercentage"
            ,"BackgroundNonBillablePercentage": "s.BackgroundNonBillablePercentage"
            ,"InteractiveNonBillablePercentage": "s.InteractiveNonBillablePercentage"
            ,"AutoscalePercentage": "s.AutoscalePercentage"
            ,"CULimitPercentage": "s.CULimitPercentage"
            ,"TotalCUUsagePercentage": "s.TotalCUUsagePercentage"
            ,"TotalCUs": "s.TotalCUs"
            ,"SKUCUByTimePoint": "s.SKUCUByTimePoint"
            ,"InteractiveDelayPercentage": "s.InteractiveDelayPercentage"
            ,"InteractiveRejectionPercentage": "s.InteractiveRejectionPercentage"
            ,"InteractiveRejectionThreshold": "s.InteractiveRejectionThreshold"
            ,"BackgroundRejectionPercentage": "s.BackgroundRejectionPercentage"
            ,"BackgroundRejectionThreshold": "s.BackgroundRejectionThreshold"
            ,"CarryOverAddedPercentage": "s.CarryOverAddedPercentage"
            ,"CarryOverBurndownPercentage": "s.CarryOverBurndownPercentage"
            ,"CarryOverCumulativePercentage": "s.CarryOverCumulativePercentage"
            ,"OverageReferenceLine": "s.OverageReferenceLine"
            ,"ExpectedBurndownInMin": "s.ExpectedBurndownInMin"
        }
    ) \
    .execute()

else:
    # else -> INSERT to gold
    print("INFO: Gold table will be created.")

    silver_df.write.mode("append").option("mergeSchema", "true").format("delta").saveAsTable(gold_table_name)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Clean Silver table
query = "DELETE FROM " + silver_table_name
spark.sql(query)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Aggregate timepoints
df = spark.sql("SELECT DISTINCT TimePoint, to_date(TimePoint) Date, to_timestamp(date_format(TimePoint, 'yyyy-mm-dd HH:00:00')) Hour  FROM FUAM_Lakehouse.capacity_metrics_by_timepoint")

# Write prepared bronze_df to silver delta table
df.write.mode("overwrite").format("delta").saveAsTable("calendar_timepoints")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
