# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "607e1068-530b-4df1-912c-750cf336eec2",
# META       "default_lakehouse_name": "FUAM_Lakehouse",
# META       "default_lakehouse_workspace_id": "bd9136d2-e45a-463d-b9be-8d1a10717acb",
# META       "known_lakehouses": [
# META         {
# META           "id": "607e1068-530b-4df1-912c-750cf336eec2"
# META         }
# META       ]
# META     },
# META     "environment": {}
# META   }
# META }

# MARKDOWN ********************

# #### Capacity Metrics (by Kind)
# by Workspace by Kind by Day
# 
# ##### Data ingestion strategy:
# <mark style="background: #D69AFE;">**MERGE**</mark>
# 
# ##### Related pipeline:
# 
# **Load_Capacity_Metrics_E2E**
# 
# ##### Source:
# 
# **Capacity Metrics** via SemPy DAX execute query function
# 
# ##### Target:
# 
# **1 Delta table** in FUAM_Lakehouse 
# - **gold_table_name** variable value

# CELL ********************

import sempy.fabric as fabric
from datetime import datetime, timedelta
import datetime as dt
import pyspark.sql.functions as f
from delta.tables import *

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# PARAMETERS CELL ********************

## Parameters
# These parameters will be overwritten while executing the notebook 
# from Load_FUAM_Data_E2E Pipeline
metric_days_in_scope = 2
metric_workspace = ""
metric_dataset = ""
display_data = False

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

## Variables
silver_table_name = "FUAM_Staging_Lakehouse.capacity_metrics_by_item_kind_by_day_silver"
gold_table_name = "capacity_metrics_by_item_kind_by_day"
gold_table_name_with_prefix = f"Tables/{gold_table_name}"

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Check Table Status
version = ''
  
try: 
      check_table_structure_query = """DEFINE    MPARAMETER 'DefaultCapacityID' = "0000000-0000-0000-0000-00000000"
                                      EVALUATE   SUMMARIZECOLUMNS("Background billable CU %", [Background billable CU %]    )"""
      check_table_structure_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query)
      print("Test for v44 successful")
      version = 'v44'
except:
      print("Test for v44 failed")
  
if version == '':
    try:
        check_table_structure_query = """EVALUATE ROW("Background billable CU %", 'All Measures'[Background billable CU %])"""
        check_table_structure_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query)
        print("Test for v40 successful")
        version = 'v40'
    except:
        print("Test for v40 failed")
  
if version == '':
    try:
        check_table_structure_query_alternative = """EVALUATE ROW("xBackground__", 'All Measures'[xBackground %])"""
        check_table_structure_df_alternative = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=check_table_structure_query_alternative)
        version = 'v37'
        print("Test for v37 successful")
    except:
        print("Test for v37 failed")
  
  
if version != '':
    print( f'Version {version} is valid')
else: 
    print("ERROR: Capacity Metrics data structure is not compatible or connection to capacity metrics is not possible.")
    exit

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Fetch capacities from connected capacity metrics app
if version in ['v44', 'v40']:
  capacity_query = """EVALUATE SELECTCOLUMNS (    Capacities, "capacity Id", Capacities[capacity Id] , "state" , Capacities[state] )"""
else:
  capacity_query = """EVALUATE SELECTCOLUMNS (    Capacities, "capacity Id", Capacities[CapacityId] , "state" , Capacities[state] )"""
capacities = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=capacity_query)
capacities.columns = ['CapacityId', 'State']
capacities = spark.createDataFrame(capacities)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

if display_data:
    display(capacities)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Iterate days
def iterate_dates(start_date, end_date):
    # Init array
    dates = []
    # Convert string inputs to datetime objects
    start = dt.datetime.strptime(start_date, '%Y-%m-%d')
    end = dt.datetime.strptime(end_date, '%Y-%m-%d')
    
    # Initialize current date as start date
    current_date = start.date()
    
    while current_date <= end.date():

        dates.append(
            {
                "date": current_date,
                "year": current_date.year,
                "month": current_date.month,
                "day": current_date.day
            })
        # Move to the next day
        current_date += dt.timedelta(days=1)

    return dates

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Clean Silver table
try:
    query = "DELETE FROM " + silver_table_name
    spark.sql(query)

except Exception as ex:
    print("Silver table doesn't exist yet.") 

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Iterate capacities and days
for cap in capacities.collect():
    capacity_id = cap[0]
    
    print(f"INFO: Scoped CapacityId: {capacity_id}")

    try:
        # Get today's date
        today = datetime.now()

        # Calculate the dates between today and days_in_scope
        days_ago = today - timedelta(days=metric_days_in_scope)

        # Format dates in 'yyyy-mm-dd'
        today_str = today.strftime('%Y-%m-%d')
        days_ago_str = days_ago.strftime('%Y-%m-%d')

        date_array = iterate_dates(days_ago_str, end_date=today_str)

        # Iterate days for current capacity
        for date in date_array:

            year = date['year']
            month = date['month']
            day = date['day']
            date_label = str(year) + '-' + str(month) + '-' + str(day)
            print(f"INFO: Get data for CapacityId: {capacity_id}")

            dax_query_v44 = f""" DEFINE 
                    MPARAMETER 'CapacitiesList' = {{\"{capacity_id}\"}}

                    VAR __DS0FilterTable = 
                                        FILTER(
                                            KEEPFILTERS(VALUES('Metrics By Item Operation And Day'[Date])),
                                            'Metrics By Item Operation And Day'[Date] = DATE({year}, {month}, {day})
                                        )

                    VAR __DS0Core = 
                                    SUMMARIZECOLUMNS(
                                            'Metrics By Item Operation And Day'[Capacity Id],
                                            'Metrics By Item Operation And Day'[Workspace Id],
                                            'Metrics By Item Operation And Day'[Date],
                                            'Items'[Item Kind],
                                            FILTER(Capacities, Capacities[capacity Id] =  \"{capacity_id}\" ),
                                            __DS0FilterTable,
											"S_Dur", SUM('Metrics By Item Operation And Day'[Duration (s)]),
											"S_CU", SUM('Metrics By Item Operation And Day'[CU (s)]),
											"TH_M", SUM('Metrics By Item Operation And Day'[Throttling (min)]),
											"C_U", SUM('Metrics By Item Operation And Day'[Users]),
											"C_SO", SUM('Metrics By Item Operation And Day'[Successful operations]),
											"C_RO", SUM('Metrics By Item Operation And Day'[Rejected operations]),
											"C_O", SUM('Metrics By Item Operation And Day'[Operations]),
											"C_IO", SUM('Metrics By Item Operation And Day'[Invalid operations]),
											"C_FO", SUM('Metrics By Item Operation And Day'[Failed operations]),
											"C_CO", SUM('Metrics By Item Operation And Day'[Cancelled operations])
                                            )
                    EVALUATE
                        ADDCOLUMNS(
                            FILTER(__DS0Core, [S_CU] > 0),
                            "DateKey", FORMAT([Date], "yyyymmdd")
                        ) ORDER BY [S_CU] DESC  """

            dax_query_v40 = f"""
                 DEFINE 
                    MPARAMETER 'CapacityID' = \"{capacity_id}\"

                    VAR __DS0FilterTable = 
                                        FILTER(
                                            KEEPFILTERS(VALUES('Metrics By Item Operation And Day'[Date])),
                                            'Metrics By Item Operation And Day'[Date] = DATE({year}, {month}, {day})
                                        )

                    VAR __DS0Core = 
                                    SUMMARIZECOLUMNS(
                                            'Metrics By Item Operation And Day'[Capacity Id],
                                            'Metrics By Item Operation And Day'[Workspace Id],
                                            'Metrics By Item Operation And Day'[Date],
                                            'Items'[Item Kind],
                                            FILTER(Capacities, Capacities[Capacity Id] = \"{capacity_id}\"  ),
                                            __DS0FilterTable,
											"S_Dur", SUM('Metrics By Item Operation And Day'[Duration (s)]),
											"S_CU", SUM('Metrics By Item Operation And Day'[CU (s)]),
											"TH_M", SUM('Metrics By Item Operation And Day'[Throttling (min)]),
											"C_U", SUM('Metrics By Item Operation And Day'[Users]),
											"C_SO", SUM('Metrics By Item Operation And Day'[Successful operations]),
											"C_RO", SUM('Metrics By Item Operation And Day'[Rejected operations]),
											"C_O", SUM('Metrics By Item Operation And Day'[Operations]),
											"C_IO", SUM('Metrics By Item Operation And Day'[Invalid operations]),
											"C_FO", SUM('Metrics By Item Operation And Day'[Failed operations]),
											"C_CO", SUM('Metrics By Item Operation And Day'[Cancelled operations])
                                            )
                    EVALUATE
                        ADDCOLUMNS(
                            FILTER(__DS0Core, [S_CU] > 0),
                            "DateKey", FORMAT([Date], "yyyymmdd")
                        ) ORDER BY [S_CU] DESC 
                    """


            dax_query_v37 = f"""
                DEFINE 
                    MPARAMETER 'CapacityID' = \"{capacity_id}\"

                    VAR __DS0FilterTable = 
                                        FILTER(
                                            KEEPFILTERS(VALUES('MetricsByItemandDay'[Date])),
                                            'MetricsByItemandDay'[Date] = DATE({year}, {month}, {day})
                                        )

                    VAR __DS0Core = 
                                    SUMMARIZECOLUMNS(
                                            Capacities[capacityId],
                                            Items[WorkspaceId],
                                            'MetricsByItemandDay'[Date],
                                            'Items'[ItemKind],
                                            FILTER(Capacities, Capacities[capacityId] = \"{capacity_id}\" ),
                                            __DS0FilterTable,
                                            "S_Dur", SUM('MetricsByItemandDay'[sum_duration]),
                                            "S_CU", SUM('MetricsByItemandDay'[sum_CU]),
                                            "TH_M", SUM('MetricsByItemandDay'[Throttling (min)]),
                                            "C_U", SUM('MetricsByItemandDay'[count_users]),
                                            "C_SO", SUM('MetricsByItemandDay'[count_successful_operations]),
                                            "C_RO", SUM('MetricsByItemandDay'[count_rejected_operations]),
                                            "C_O", SUM('MetricsByItemandDay'[count_operations]),
                                            "C_IO", SUM('MetricsByItemandDay'[count_Invalid_operations]),
                                            "C_FO", SUM('MetricsByItemandDay'[count_failure_operations]),
                                            "C_CO", SUM('MetricsByItemandDay'[count_cancelled_operations])
                                            )
                    EVALUATE
                        ADDCOLUMNS(
                            FILTER(__DS0Core, [S_CU] > 0),
                            "DateKey", FORMAT([Date], "yyyymmdd")
                        ) ORDER BY [S_CU] DESC
                    """

            dax_query = ""
            # Choose query
            if version == 'v44':
                dax_query = dax_query_v44
            elif version == 'v40':
                dax_query = dax_query_v40
            elif version == 'v37':
                dax_query = dax_query_v37

           
            
            # Execute DAX query
            capacity_df = fabric.evaluate_dax(workspace=metric_workspace, dataset=metric_dataset, dax_string=dax_query)
            capacity_df.columns = ['CapacityId', 'WorkspaceId', 'Date',  
                                    'ItemKind', 'DurationInSec','TotalCUs', 'ThrottlingInMin', 
                                    'UserCount','SuccessOperationCount', 'RejectedOperationCount','OperationCount',
                                    'InvalidOperationCount','FailureOperationCount','CancelledOperationCount', 'DateKey']
            
            if not(capacity_df.empty):
                # Transfer pandas df to spark df
                capacity_df = spark.createDataFrame(capacity_df)
                
                if display_data:
                    display(capacity_df)

                # Write prepared bronze_df to silver delta table
                print(f"INFO: Appending data. Capacity: {capacity_id} on Date: {date_label}")
                capacity_df.write.mode("append").option("mergeSchema", "true").format("delta").saveAsTable(silver_table_name)
            else:
                print(f"INFO: No data for CapacityId: {capacity_id} on Date: {date_label}")

    except Exception as ex:
        print('ERROR: Exception for CapacityId: ' + capacity_id + '. ->' + str(ex))
        continue

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Get Silver table data
query = "SELECT * FROM " + silver_table_name
silver_df = spark.sql(query)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Check if gold table exists
if spark._jsparkSession.catalog().tableExists('FUAM_Lakehouse', gold_table_name):
    # if exists -> MERGE to gold
    print("INFO: Gold table exists and will be merged.")

    gold_df = DeltaTable.forPath(spark, gold_table_name_with_prefix)
    # Merge silver (s = source) to gold (t = target)
    gold_df.alias('t') \
    .merge(
        silver_df.alias('s'),
        "s.CapacityId = t.CapacityId AND s.WorkspaceId = t.WorkspaceId AND s.Date = t.Date AND s.ItemKind = t.ItemKind"
    ) \
    .whenMatchedUpdate(set =
        {
             "DurationInSec": "s.DurationInSec"
            ,"TotalCUs": "s.TotalCUs"
            ,"ThrottlingInMin": "s.ThrottlingInMin"
            ,"UserCount": "s.UserCount"
            ,"SuccessOperationCount": "s.SuccessOperationCount"
            ,"RejectedOperationCount": "s.RejectedOperationCount"
            ,"OperationCount": "s.OperationCount"
            ,"InvalidOperationCount": "s.InvalidOperationCount"
            ,"FailureOperationCount": "s.FailureOperationCount"
            ,"CancelledOperationCount": "s.CancelledOperationCount"
        }
    ) \
    .whenNotMatchedInsert(values =
        {
             "CapacityId": "s.CapacityId"
            ,"WorkspaceId": "s.WorkspaceId"
            ,"Date": "s.Date"
            ,"ItemKind": "s.ItemKind"
            ,"DurationInSec": "s.DurationInSec"
            ,"TotalCUs": "s.TotalCUs"
            ,"ThrottlingInMin": "s.ThrottlingInMin"
            ,"UserCount": "s.UserCount"
            ,"SuccessOperationCount": "s.SuccessOperationCount"
            ,"RejectedOperationCount": "s.RejectedOperationCount"
            ,"OperationCount": "s.OperationCount"
            ,"InvalidOperationCount": "s.InvalidOperationCount"
            ,"FailureOperationCount": "s.FailureOperationCount"
            ,"CancelledOperationCount": "s.CancelledOperationCount"
            ,"DateKey": "s.DateKey"
        }
    ) \
    .execute()

else:
    # else -> INSERT to gold
    print("INFO: Gold table will be created.")

    silver_df.write.mode("append").option("mergeSchema", "true").format("delta").saveAsTable(gold_table_name)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Clean Silver table
query = "DELETE FROM " + silver_table_name
spark.sql(query)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
